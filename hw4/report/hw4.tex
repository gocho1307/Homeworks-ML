\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext,newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{subfig,graphicx}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multirow}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
\usepackage{float}

\newcommand{\ind}{\perp\!\!\!\perp}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

% BACKGROUND BOX COLORS
\definecolor{byellow}{HTML}{e0b400}
\definecolor{bmint}{HTML}{00c49a}

% \highlight[<colour>]{<stuff>}
\newcommand{\highlight}[2][yellow]{\mathchoice
  {\colorbox{#1}{$\displaystyle#2$}}
  {\colorbox{#1}{$\textstyle#2$}}
  {\colorbox{#1}{$\scriptstyle#2$}}
  {\colorbox{#1}{$\scriptscriptstyle#2$}}}

\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=6pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}
\begin{center}
\large{Aprendizagem 2023} \\
Homework IV -- Group 28 \\
\vskip 0.3cm
Gonçalo Bárias (ist1103124) \& Raquel Braunschweig (ist1102624)\vskip 1cm

\large{\textbf{Part I}: Pen and Paper}\normalsize
\end{center}

\noindent Given the following observations, $\left\{\begin{pmatrix} 1 \\ 0.6 \\ 0.1 \end{pmatrix}, \begin{pmatrix} 0 \\ -0.4 \\ 0.8 \end{pmatrix}, \begin{pmatrix} 0 \\ 0.2 \\ 0.5 \end{pmatrix},
\begin{pmatrix} 1 \\ 0.4 \\ -0.1 \end{pmatrix}\right\}$.

\vskip 0.2cm
\noindent Consider a Bayesian clustering that assumes $\{y_1\} \ind \{y_2, y_3\}$, two clusters following a Bernoulli distribution on $y_1$ ($p_1$ and $p_2$), a multivariate Gaussian on $\{y_2, y_3\}$ ($N_1$ and $N_2$),
and the following initial mixture:

\vskip -0.3cm
\begin{equation*}
    \pi_1 = 0.5 \quad , \quad \pi_2 = 0.5
\end{equation*}
\begin{equation*}
    p_1 = P(y_1 = 1) = 0.3 \quad , \quad p_2 = P(y_1 = 1) = 0.7
\end{equation*}
\begin{equation*}
    \mathcal{N}_1 \left(\boldsymbol{\mu}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \mathbf{\Sigma}_1 = \begin{pmatrix} 2 & 0.5 \\ 0.5 & 2 \end{pmatrix}\right) \quad
    , \quad \mathcal{N}_2 \left(\boldsymbol{\mu}_2 = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \mathbf{\Sigma}_2 = \begin{pmatrix} 1.5 & 1 \\ 1 & 1.5 \end{pmatrix}\right)
\end{equation*}

\vskip 0.2cm
\begin{enumerate}[leftmargin=\labelsep]
    \item \textbf{Perform one epoch of the EM clustering algorithm and determine the new parameters.}\\
          \textbf{\textit{Hint:} we suggest you to use numpy and scipy, however disclose the intermediary results step by step.}

          \vskip 0.3cm
          The EM (Expectation-Maximization) algorithm has four major steps:
          Initialization, Expectation, Maximization and Evaluate.

          \textbf{Initialization}

          We'll start by labeling each observation:

          $$
              x_1 = \begin{pmatrix} 1 \\ 0.6 \\ 0.1 \end{pmatrix}
              \quad,\quad
              x_2 = \begin{pmatrix} 0 \\ -0.4 \\ 0.8 \end{pmatrix}
              \quad,\quad
              x_3 = \begin{pmatrix} 0 \\ 0.2 \\ 0.5 \end{pmatrix}
              \quad,\quad
              x_4 = \begin{pmatrix} 1 \\ 0.4 \\ -0.1 \end{pmatrix}
          $$

          From the statement we have the following initial parameters, $p_1$, $p_2$, $\mu_1$, $\mu_2$, $\Sigma_1$,
          $\Sigma_2$, $\pi_1$ and $\pi_2$:

          \begin{center}
              \captionsetup{type=table}
              \begin{tabular}{c|cccc}
                  Cluster & $p$ & $\mu$ & $\Sigma$ & $\pi$            \\
                  \hline
                  \colorbox{bmint}{Cluster 1}                         &
                  0.3                                                 &
                  $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$              &
                  $\begin{pmatrix} 2 & 0.5 \\ 0.5 & 2 \end{pmatrix}$  &
                  0.5                                                 \\
                  \colorbox{byellow}{Cluster 2}                       &
                  0.7                                                 &
                  $\begin{pmatrix} 0 \\ 0 \end{pmatrix}$              &
                  $\begin{pmatrix} 1.5 & 1 \\ 1 & 1.5 \end{pmatrix}$  &
                  0.5                                                 \\
              \end{tabular}
              \captionof{table}{Initial parameters for the 2 clusters}
              \label{ex1-initial-params-table}
          \end{center}

          \textbf{Expectation (E-step)}

          Considering $\{y_1\} \ind \{y_2, y_3\}$ we know the posterior probability, $P(c_k | x_i)$, is given by Baye's rule:

          \begin{equation}\label{ex1-posterior}
              \gamma_{k,i} = P(c_k | x_i) = \frac{P(y_1,y_2,y_3|c_k)P(c_k)}{P(y_1,y_2,y_3)} = \frac{P(y_1|c_k)P(y_2,y_3|c_k)P(c_k)}{P(y_1)P(y_2,y_3)}
          \end{equation}

          % TODO: explain normalization

          The variable $y_1$ follows a Bernoulli distribution ($y_1 \sim \text{Bern}\left(p=p_k\right)$), and so the likelihoods,\\
          $P(y_1=0|c_k)$ and $P(y_1=1|c_k)$, can be calculated for each cluster:

          \begin{align*}
              P(y_1 = 0 | c_1) = 1 - p_1 = 1 - 0.3 = 0.7 & \qquad P(y_1 = 0 | c_2) = 1 - p_2 = 1 - 0.7 = 0.3 \\
              P(y_1 = 1 | c_1) = p_1 = 0.3               & \qquad P(y_1 = 1 | c_2) = p_2 = 0.7
          \end{align*}

          We know the likelihood, $P(y_2,y_3|c_k)$, follows a multivariate Gaussian, and so it is given by
          (considering $d = 2$, since we are working in two dimensions):

          \begin{equation}\label{ex1-likelihood-multivariate}
              P(y_2=a,y_3=b|c_k) = \mathcal{N}_k(y_2,y_3|\mu_k, \Sigma_k)
              = \frac{
              \exp\left(-\frac{1}{2} \left(\begin{bmatrix} a \\ b \end{bmatrix} - \mu_k\right)^T
              \Sigma_k^{-1} \left(\begin{bmatrix} a \\ b \end{bmatrix} - \mu_k\right)\right)}{(2\pi)^{d/2} \times |\Sigma_k|^{1/2}}
          \end{equation}

          We now have all the building blocks to calculate the posterior probabilities
          for each combination of observation, $x_i$ and cluster, $c_k$.

          % TODO: do algo for each observation

          \textbf{Maximization (M-step)}

          For each cluster, $c_k$, we will calculate the following in order to update the parameters:

          \begin{equation*}
              N_k = \sum_i \gamma_{k,i}
          \end{equation*}
          \begin{equation*}
              p_k' = \frac{1}{N_k} \sum_{i} \gamma_{k,i} \cdot {x_i}_{[y_1]}
          \end{equation*}
          \begin{equation*}
              \mu_k' = \frac{1}{N_k} \sum_{i} \gamma_{k,i} \cdot {x_i}_{[y_2 \land y_3]}
          \end{equation*}
          \begin{equation*}
              \Sigma_k' = \frac{1}{N_k} \sum_{i} \gamma_{k,i} \cdot \left({x_i}_{[y_2 \land y_3]} - \mu_k'\right) \cdot ({x_i}_{[y_2 \land y_3]} - \mu_k')^T
          \end{equation*}

          Considering $N = \sum_k N_k$, we can also update the priors:

          \begin{equation*}
              \pi_k' = \frac{N_k}{N}
          \end{equation*}

          We can now update the values for both clusters using the previous equations:

          \begin{equation*}
              \highlight[bmint]{N_1} = \sum_{i} \gamma_{1,i} = \gamma_{1,1} + \gamma_{1,2} + \gamma_{1,3} + \gamma_{1,4} = 1.54467
          \end{equation*}
          \begin{equation*}
              \highlight[byellow]{N_2} = \sum_{i} \gamma_{2,i} = \gamma_{2,1} + \gamma_{2,2} + \gamma_{2,3} + \gamma_{2,4} = 2.45533
          \end{equation*}

          % Cluster 1
          \begin{center}
              \textbf{\colorbox{bmint}{Cluster 1}}
          \end{center}

          \begin{equation*}
              p_1' = \frac{1}{N_1} \sum_{i} \gamma_{1,i} \cdot {x_i}_{[y_1]}
                   = \frac{\gamma_{1,1} \cdot 1
                          + \gamma_{1,2} \cdot 0
                          + \gamma_{1,3} \cdot 0
                          + \gamma_{1,4} \cdot 1}{1.54467}
                   = 0.23404
          \end{equation*}

          \begin{equation*}
              \mu_1' = \frac{1}{N_1} \sum_{i} \gamma_{1,i} \cdot {x_i}_{[y_2 \land y_3]}
                     = \frac{\gamma_{1,1} \cdot \begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix}
                            + \gamma_{1,2} \cdot \begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix}
                            + \gamma_{1,3} \cdot \begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix}
                            + \gamma_{1,4} \cdot \begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix}}{1.54467}
                     = \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}
          \end{equation*}

          \begin{align*}
              \Sigma_1' & = \frac{1}{N_1} \sum_{i} \gamma_{1,i} \cdot \left({x_i}_{[y_2 \land y_3]} - \mu_1'\right) \cdot ({x_i}_{[y_2 \land y_3]} - \mu_1')^T \\
                        & = \frac{1}{1.54467} \times \Bigg[ \Bigg.
                            \gamma_{1,1} \cdot \left(\begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right)^T \\
                        & + \gamma_{1,2} \cdot \left(\begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right)^T \\
                        & + \gamma_{1,3} \cdot \left(\begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right)^T \\
                        & + \gamma_{1,4} \cdot \left(\begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix} - \begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}\right)^T
                        \Bigg. \Bigg] \\
                        & = \begin{pmatrix} 0.14137 & -0.10541 \\ -0.10541 & 0.09605 \end{pmatrix}
          \end{align*}

          \begin{equation*}
              \pi_1' = \frac{N_1}{N} = \frac{N_1}{N_1 + N_2} = \frac{1.54467}{1.54467 + 2.45533} = 0.38617
          \end{equation*}

          \vspace{10pt}
          \hrule
          \vspace{5pt}

          % Cluster 2
          \begin{center}
              \textbf{\colorbox{byellow}{Cluster 2}}
          \end{center}

          \begin{equation*}
              p_2' = \frac{1}{N_2} \sum_{i} \gamma_{2,i} \cdot {x_i}_{[y_1]}
                   = \frac{\gamma_{2,1} \cdot 1
                          + \gamma_{2,2} \cdot 0
                          + \gamma_{2,3} \cdot 0
                          + \gamma_{2,4} \cdot 1}{2.45533}
                   = 0.66732
          \end{equation*}

          \begin{equation*}
              \mu_2' = \frac{1}{N_2} \sum_{i} \gamma_{2,i} \cdot {x_i}_{[y_2 \land y_3]}
                     = \frac{\gamma_{2,1} \cdot \begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix}
                            + \gamma_{2,2} \cdot \begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix}
                            + \gamma_{2,3} \cdot \begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix}
                            + \gamma_{2,4} \cdot \begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix}}{2.45533}
                     = \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}
          \end{equation*}

          \begin{align*}
              \Sigma_2' & = \frac{1}{N_2} \sum_{i} \gamma_{2,i} \cdot \left({x_i}_{[y_2 \land y_3]} - \mu_2'\right) \cdot ({x_i}_{[y_2 \land y_3]} - \mu_2')^T \\
                        & = \frac{1}{2.45533} \times \Bigg[ \Bigg.
                            \gamma_{2,1} \cdot \left(\begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.6 \\ 0.1 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right)^T \\
                        & + \gamma_{2,2} \cdot \left(\begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} -0.4 \\ 0.8 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right)^T \\
                        & + \gamma_{2,3} \cdot \left(\begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.2 \\ 0.5 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right)^T \\
                        & + \gamma_{2,4} \cdot \left(\begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right) \cdot \left(\begin{pmatrix} 0.4 \\ -0.1 \end{pmatrix} - \begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}\right)^T
                        \Bigg. \Bigg] \\
                        & = \begin{pmatrix} 0.10829 & -0.08865 \\ -0.08865 & 0.10412 \end{pmatrix}
          \end{align*}

          \begin{equation*}
              \pi_2' = \frac{N_2}{N} = \frac{N_2}{N_1 + N_2} = \frac{2.45533}{1.54467 + 2.45533} = 0.61383
          \end{equation*}

          \textbf{Evaluate the log likelihood}

          Since we are only performing one epoch of the EM clustering algorithm, we can skip this step.

          \textbf{Conclusion}

          After performing one epoch of the EM clustering algorithm, we end up with the following updated parameters for each cluster:

          \begin{center}
              \captionsetup{type=table}
              \begin{tabular}{c|cccc}
                  Cluster & $p'$ & $\mu'$ & $\Sigma'$ & $\pi'$                              \\
                  \hline
                  \colorbox{bmint}{Cluster 1}                                               &
                  0.23404                                                                   &
                  $\begin{pmatrix} 0.02651 \\ 0.50713 \end{pmatrix}$                        &
                  $\begin{pmatrix} 0.14137 & -0.10541 \\ -0.10541 & 0.09605 \end{pmatrix}$  &
                  0.38617                                                                   \\
                  \colorbox{byellow}{Cluster 2}                                             &
                  0.66732                                                                   &
                  $\begin{pmatrix} 0.30914 \\ 0.21042 \end{pmatrix}$                        &
                  $\begin{pmatrix} 0.10829 & -0.08865 \\ -0.08865 & 0.10412 \end{pmatrix}$  &
                  0.61383                                                                   \\
              \end{tabular}
              \captionof{table}{Updated parameters for the 2 clusters}
              \label{ex1-updated-params-table}
          \end{center}

    \item \textbf{Given the new observation, $x_{new} = \begin{bmatrix} 1 & 0.3 & 0.7 \end{bmatrix}^T$, determine the cluster memberships (posteriors).}

          \vskip 0.3cm
          Raquel

    \item \textbf{Performing a hard assignment of observations to clusters under a ML assumption, identify the silhouette of both clusters under a Manhattan distance.}

          \vskip 0.3cm
          Raquel

    \item \textbf{Knowing the purity of the clustering solution is 0.75, identify the number of possible classes (ground truth).}

          \vskip 0.3cm
          Raquel
\end{enumerate}

\vskip 0.5cm

\begin{center}
\large{\textbf{Part II}: Programming and critical analysis}\normalsize
\end{center}

\noindent Recall the \texttt{column\_diagnosis.arff} dataset from previous homeworks. For the following exercises,
normalize the data using sklearn's \texttt{MinMaxScaler}.

\begin{enumerate}[leftmargin=\labelsep]
    \item \textbf{Using \texttt{sklearn}, apply \textit{k}-means clustering fully unsupervisedly on the normalized data with
          $k \in \{2,3,4,5\}$ (\textnormal{random = 0} and remaining parameters as default). Assess the silhouette and purity of the produced solutions.}

          \vskip 0.3cm
          Gonçalo

    \item \textbf{Consider the application of PCA after the data normalization:}

    \begin{enumerate}
        \item \textbf{Identify the variability explained by the top two principal components.}

              \vskip 0.3cm
              Raquel

        \item \textbf{For each one of these two components, sort the input variables by relevance by
              inspecting the absolute weights of the linear projection.}

              \vskip 0.3cm
              Raquel
    \end{enumerate}

    \item \textbf{Visualize side-by-side the data using: i) the ground diagnoses, and ii) the \textit{previously} learned
          $k = 3$ clustering solution. To this end, projected the normalized data onto a 2-dimensional data
          space using PCA and then color observations using the reference and cluster annotations.}

          \vskip 0.3cm
          Raquel

    \item \textbf{Considering the results from questions (1) and (3), identify two ways on how clustering can
          be used to characterize the population of ill and healthy individuals.}

          \vskip 0.3cm
          Raquel
\end{enumerate}
\end{document}
