\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext,newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{subfig,graphicx}
\usepackage[colorlinks=true]{hyperref}
\usepackage{multirow}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
\usepackage{float}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}
\begin{center}
\large{Aprendizagem 2023}\\
Homework III -- Group 28\\
\vskip 0.3cm
Gonçalo Bárias (ist1103124) \& Raquel Braunschweig (ist1102624)\vskip 1cm

\large{\textbf{Part I}: Pen and Paper}\normalsize
\end{center}

\begin{enumerate}[leftmargin=\labelsep]
    \item \textbf{Consider the problem of learning a regression model from 4 bivariate observations} \\

          \vskip -0.2cm
          \textbf{$\left\{\begin{pmatrix} 0.7 \\ -0.3 \end{pmatrix}, \begin{pmatrix} 0.4 \\ 0.5 \end{pmatrix}, \begin{pmatrix} -0.2 \\ 0.8 \end{pmatrix},
          \begin{pmatrix} -0.4 \\ 0.3 \end{pmatrix}\right\}$ with targets (0.8, 0.6, 0.3, 0.3)}.

    \begin{enumerate}
        \item \textbf{Given the radial basis function, $\varphi_j(x) = \text{exp}({ \frac{-\| \mathbf{x} - \mathbf{c}_j \|^2}{2} })$ that transforms
              the original space onto a new space characterized by the similarity of the original observastions to the following data points
              $\left\{ c_1 = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, c_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}, c_3 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}\right\}$. \\
              Learn the Ridge regression ($l_2$ regularization) using the closed solution with $\lambda$ = 0.1.}

              \vskip 0.3cm
              Gonçalo

        \item \textbf{Compute the training RMSE for the learnt regression.}

              \vskip 0.3cm
              Gonçalo
    \end{enumerate}

    \item \textbf{Consider a MLP classifier of three outcomes - A, B and C - characterized by the weights,} \\

          \vskip -0.2cm
          $W^{[1]} = \begin{pmatrix} 1 1 1 1 \\ 1 1 2 1 \\ 1 1 1 1\end{pmatrix}, b^{[1]} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix},
          W^{[2]} = \begin{pmatrix} 1 4 1 \\ 1 1 1 \end{pmatrix}, b^{[2]} = \begin{pmatrix} 1 \\ 1 \end{pmatrix},
          W^{[3]} = \begin{pmatrix} 1 1 \\ 3 1 \\ 1 1\end{pmatrix}, b^{[3]} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ \\

          \textbf{the activation \[ f(x) = \frac{{e^{0.5x - 2} - e^{-0.5x + 2}}}{{e^{0.5x - 2} + e^{-0.5x + 2}}} = \tanh(0.5x - 2) \] for every unit, and squared error
          loss $\frac{1}{2} \|\mathbf{z} - \hat{\mathbf{z}}\|^{2}_{2}$. Perform one back gradient descent update (with learning rate $\eta = 0.1$) for training
          obvervations $x_1 = \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix}$ and $x_2 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ -1 \end{pmatrix}$ with targets B and A,
          respectively.}

          \vskip 0.3cm

          To help with the calculations and as suggested in the prompt, we used NumPy to facilitate the calculus of this question. You can find the code in the Jupyter notebook \texttt{hw3.ipynb}.
          
          To begin, we initiate the process with the \textbf{forward pass}. Below are the key equations for this step:

            \begin{equation}\label{ex2-z}
                \mathbf{z}^{[n]} = \mathbf{W}^{[n]} \cdot \mathbf{X}^{{n-1}} + \mathbf{b}^{[n]} 
            \end{equation}

            \begin{equation}\label{ex2-x}
                \mathbf{x}^{[n]} = f(\mathbf{z}^{[n]})
            \end{equation}

          We will commence with $x_1$:

            \begin{align*}
                z^{[1]}_1 &= {W}^{[1]} \cdot {X}^{0}_1 + {b}^{[1]} = \begin{pmatrix} 1 1 1 1 \\ 1 1 2 1 \\ 1 1 1 1\end{pmatrix} \cdot  \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} +
                 \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} = \begin{pmatrix} 5 \\ 6 \\ 5\end{pmatrix} \\
                {x}^{[1]}_1 &= f({z}^{[1]}_1) \approx \begin{pmatrix} 0.46212 \\ 0.76159 \\ 0.46212\end{pmatrix} \\
            \end{align*}

            \begin{align*}
              z^{[2]}_1 &= {W}^{[2]} \cdot {X}^{1}_1 + {b}^{[2]} = \begin{pmatrix} 1 4 1 \\ 1 1 1\end{pmatrix} \cdot  \begin{pmatrix} 0.46212 \\ 0.76159 \\ 0.46212 \end{pmatrix} +
               \begin{pmatrix} 1 \\ 1\end{pmatrix} \approx \begin{pmatrix} 4.97061 \\ 2.68583\end{pmatrix} \\
              {x}^{[2]}_1 &= f({z}^{[2]}_1) \approx \begin{pmatrix} 0.45048 \\ -0.57642\end{pmatrix} \\
            \end{align*}

            \begin{align*}
              z^{[3]}_1 &= {W}^{[3]} \cdot {X}^{2}_1 + {b}^{[3]} = \begin{pmatrix} 1 1 \\ 3 1 \\ 1 1\end{pmatrix} \cdot  \begin{pmatrix} 0.45048 \\ -0.57642\end{pmatrix} +
               \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} \approx \begin{pmatrix} 0.87406 \\ 1.77503 \\ 0.87406\end{pmatrix} \\
              {x}^{[3]}_1 &= f({z}^{[3]}_1) \approx \begin{pmatrix} -0.9159 \\ -0.80494 \\ -0.9159\end{pmatrix} \\
            \end{align*}


          Next in line is $x_2$:

          \begin{align*}
            z^{[1]}_2 &= {W}^{[1]} \cdot {X}^{0}_2 + {b}^{[1]} = \begin{pmatrix} 1 1 1 1 \\ 1 1 2 1 \\ 1 1 1 1\end{pmatrix} \cdot  \begin{pmatrix} 1 \\ 0 \\ 0 \\ -1 \end{pmatrix} +
             \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} = \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} \\
            {x}^{[1]}_2 &= f({z}^{[1]}_2) \approx \begin{pmatrix} -0.90515 \\ -0.90515 \\ -0.90515\end{pmatrix} \\
          \end{align*}

          \begin{align*}
            z^{[2]}_2 &= {W}^{[2]} \cdot {X}^{1}_2 + {b}^{[2]} = \begin{pmatrix} 1 4 1 \\ 1 1 1\end{pmatrix} \cdot  \begin{pmatrix} -0.90515 \\ -0.90515 \\ -0.90515 \end{pmatrix} +
            \begin{pmatrix} 1 \\ 1\end{pmatrix} \approx \begin{pmatrix} -4.43089 \\ -1.71544\end{pmatrix} \\
            {x}^{[2]}_2 &= f({z}^{[2]}_2) \approx \begin{pmatrix} -0.99956 \\ -0.99343\end{pmatrix} \\
          \end{align*}

          \begin{align*}
            z^{[3]}_2 &= {W}^{[3]} \cdot {X}^{2}_2 + {b}^{[3]} = \begin{pmatrix} 1 1 \\ 3 1 \\ 1 1\end{pmatrix} \cdot  \begin{pmatrix} -0.99956 \\ -0.99343\end{pmatrix} +
            \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} \approx \begin{pmatrix} -0.993 \\ -2.99212 \\ -0.993\end{pmatrix} \\
            {x}^{[3]}_2 &= f({z}^{[3]}_2) \approx \begin{pmatrix} -0.98652 \\ -0.99816 \\ -0.98652\end{pmatrix} \\
          \end{align*}

          Let's initiate the \textbf{backpropagation} process. To do this, we first need to take into account the loss function, which, as per the prompt, is defined as follows:

          \begin{equation}\label{ex2-loss}
            \mathcal{L} = \frac{1}{2} \|\mathbf{z} - \hat{\mathbf{z}}\|^{2}_{2} = \frac{1}{2} (\mathbf{t} - {\mathbf{o}})^{2}
          \end{equation}

          The next step involves computing its derivative:

          \begin{equation}
            \frac{d\mathcal{L}}{dW^{[p]}} = \sum_{i=1}^{2} (\frac{d\mathcal{L}}{dx^{[p]}_i} \circ \frac{dx^{[p]}_i}{dz^{[p]}_i} \cdot \frac{dz^{[p]}_i}{dW^{[p]}} )
          \end{equation}

          This can be broken down into:

          \begin{equation*}
            \frac{d\mathcal{L}}{dx^{[p]}_i} \circ \frac{dx^{[p]}_i}{dz^{[p]}_i} = \delta^{[p]}_i 
          \end{equation*}

          \begin{equation*}
            \frac{dz^{[p]}_i}{dW^{[p]}} = (x^{[p-1]}_i)^{T} 
          \end{equation*}
          
          After computing \(\delta\), we can split it into two separate cases: if it is in the last layer \eqref{ex2-deltalastlayer}, or in non-external layers \eqref{ex2-deltanotlastlayer}:

          \begin{equation}\label{ex2-deltalastlayer}
            \delta^{[p]}_i = (x^{[p]}_i - z_i) \circ  sech^{2}(0.5*z^{[p]}_i - 2) * 0.5
          \end{equation}

          \begin{equation}\label{ex2-deltanotlastlayer}
            \delta^{[p]}_i = (W^{[p+1]^{T}} \cdot \delta^{[p+1]}_i) \circ  sech^{2}(0.5*z^{[p]}_i - 2) * 0.5
          \end{equation}

          \textbf{Therefore,} the derivitave of the loss fuction is given by:

          \begin{equation}\label{ex2-derivate-loss-smiplified}
            \frac{d\mathcal{L}}{dW^{[p]}} = \sum_{i=1}^{2} (\delta^{[p]}_i \cdot (x^{[p-1]}_i)^{T} )
          \end{equation}

          \textbf{Since,} the $tanh$ function variates from -1 to 1 and since the targets for $x_1$ and $x_2$ are B and A, respectively, we can conclude that:

          \begin{align*}
            z_1 = \begin{pmatrix} -1 \\ 1 \\ -1\end{pmatrix}, z_2 = \begin{pmatrix} 1 \\ -1 \\ -1\end{pmatrix} 
          \end{align*}
          
          \textbf{Now,} we have all we need to compute the values for the deltas, $\delta$, by replacing the equation on \ref{ex2-deltalastlayer} for $p=3$, and \ref{ex2-deltanotlastlayer} for any $p \neq 3$:

          \begin{align*}
            \delta^{[3]}_1 &= (x^{[3]}_1 - z_1) \circ  sech^{2}(0.5*z^{[3]}_1 - 2) * 0.5 =  \\
            &=  (\begin{pmatrix} -0.9159 \\ -0.80494 \\ -0.9159\end{pmatrix} - \begin{pmatrix} -1 \\ 1 \\ -1 \end{pmatrix}) \circ sech^{2}(0.5* \begin{pmatrix} 0.87406 \\ 1.77503 \\ 0.87406\end{pmatrix} * 0.5) = \\
            &= \begin{pmatrix} 0.00678 \\ -0.31773 \\ 0.00678 \end{pmatrix} 
          \end{align*}

          \begin{align*}
            \delta^{[3]}_2 &= (x^{[3]}_2 - z_2) \circ  sech^{2}(0.5*z^{[3]}_2 - 2) * 0.5 =  \\
            &=  (\begin{pmatrix} -0.98652 \\ -0.99816 \\ -0.98652\end{pmatrix} - \begin{pmatrix} 1 \\ -1 \\ -1\end{pmatrix}) \circ sech^{2}(0.5* \begin{pmatrix} -0.993 \\ -2.99212 \\ -0.993\end{pmatrix} * 0.5) = \\
            &= \begin{pmatrix} -0.0266 \\ 0 \\ 0.00018 \end{pmatrix}
          \end{align*}

          \begin{align*}
            \delta^{[2]}_1 &= (W^{[3]^{T}} \cdot \delta^{[3]}_1) \circ  sech^{2}(0.5*z^{[2]}_1 - 2) * 0.5 = \\
             &= (\begin{pmatrix} 1 1 \\ 3 1 \\ 1 1\end{pmatrix}^{T} \cdot \begin{pmatrix} 0.00678 \\ -0.31773 \\ 0.00678 \end{pmatrix}) \circ sech^{2}(0.5*\begin{pmatrix} 4.97061 \\ 2.68583\end{pmatrix} - 2) * 0.5 = \\
             &= \begin{pmatrix} -0.37448 \\ -0.10156 \end{pmatrix}
          \end{align*}

          \begin{align*}
            \delta^{[2]}_2 &= (W^{[3]^{T}} \cdot \delta^{[3]}_2) \circ  sech^{2}(0.5*z^{[2]}_2 - 2) * 0.5 = \\
             &= (\begin{pmatrix} 1 1 \\ 3 1 \\ 1 1\end{pmatrix}^{T} \cdot \begin{pmatrix} -0.0266 \\ 0 \\ -0.00018 \end{pmatrix}) \circ sech^{2}(0.5*\begin{pmatrix} 4.97061 \\ 2.68583\end{pmatrix} - 2) * 0.5 = \\
             &= \begin{pmatrix} -1 * 10^{-5} \\ -1.7 * 10^{-4} \end{pmatrix}
          \end{align*}

          \begin{align*}
            \delta^{[1]}_1 &= (W^{[2]^{T}} \cdot \delta^{[2]}_1) \circ  sech^{2}(0.5*z^{[1]}_1 - 2) * 0.5 = \\
             &= (\begin{pmatrix} 1 4 1 \\ 1 1 1 \end{pmatrix}^{T} \cdot \begin{pmatrix} -0.37448 \\ -0.10156 \end{pmatrix}) \circ sech^{2}(0.5*\begin{pmatrix}  5 \\ 6 \\ 5 \end{pmatrix} - 2) * 0.5 = \\
             &= \begin{pmatrix} -0.18719 \\ -0.33587 \\ -0.18719\end{pmatrix}
          \end{align*}

          \begin{align*}
            \delta^{[1]}_2 &= (W^{[2]^{T}} \cdot \delta^{[2]}_2) \circ  sech^{2}(0.5*z^{[1]}_2 - 2) * 0.5 = \\
             &= (\begin{pmatrix} 1 4 1 \\ 1 1 1 \end{pmatrix}^{T} \cdot \begin{pmatrix} -1 * 10^{-5} \\ -1.7 * 10^{-4} \end{pmatrix}) \circ sech^{2}(0.5*\begin{pmatrix}  1 \\ 1 \\ 1 \end{pmatrix} - 2) * 0.5 = \\
             &= \begin{pmatrix} -2 * 10^{-5} \\ -2 * 10^{-5} \\ -2 * 10^{-5}\end{pmatrix}
          \end{align*}

          \textbf{Next,} we will compute the derivatives of the loss function, employing the equation in \ref{ex2-derivate-loss-smiplified}:

          \begin{align*}
            \frac{d\mathcal{L}}{dW^{[3]}} &= \sum_{i=1}^{2} (\delta^{[3]}_i \cdot (x^{[2]}_i)^{T}) = \\
             &= (\delta^{[3]}_1 \cdot (x^{[2]}_1)^{T}) + (\delta^{[3]}_2 \cdot (x^{[2]}_2)^{T}) = \\
             &= (\begin{pmatrix} 0.00678 \\ -0.31773 \\ 0.00678 \end{pmatrix} \cdot \begin{pmatrix} 0.45048 \\ -0.57642\end{pmatrix}^{T}) + 
                 (\begin{pmatrix} -0.0266 \\ 0 \\ 0.00018 \end{pmatrix} \cdot \begin{pmatrix} -0.99956 \\ -0.99343\end{pmatrix}^{T}) = \\
             &= \begin{pmatrix} 0.02964 & 0.02252 \\ -0.14314 & 0.18315 \\ 0.00287 & -0.00408 \end{pmatrix}
          \end{align*}

          \begin{align*}
            \frac{d\mathcal{L}}{dW^{[2]}} &= \sum_{i=1}^{2} (\delta^{[2]}_i \cdot (x^{[1]}_i)^{T}) = \\
             &= (\delta^{[2]}_1 \cdot (x^{[1]}_1)^{T}) + (\delta^{[2]}_2 \cdot (x^{[1]}_2)^{T}) = \\
             &= (\begin{pmatrix} -0.37448 \\ 0.10156 \end{pmatrix} \cdot \begin{pmatrix} 0.46212 \\ 0.76159 \\ 0.46212\end{pmatrix}^{T}) + 
                 (\begin{pmatrix} -1 * 10^{-5} \\ -1.7 * 10^{-4}  \end{pmatrix} \cdot \begin{pmatrix} -0.90515 \\ -0.90515 \\ -0.90515\end{pmatrix}^{T}) = \\
             &= \begin{pmatrix} -0.17304 & -0.28519 & -0.17304\\ -0.04678 & -0.07719 & -0.04678 \end{pmatrix}
          \end{align*}

          \begin{align*}
            \frac{d\mathcal{L}}{dW^{[1]}} &= \sum_{i=1}^{2} (\delta^{[1]}_i \cdot (x^{[0]}_i)^{T}) = \\
             &= (\delta^{[1]}_1 \cdot (x^{[0]}_1)^{T}) + (\delta^{[1]}_2 \cdot (x^{[0]}_2)^{T}) = \\
             &= (\begin{pmatrix} -0.18719 \\ -0.33587 \\ -0.18719 \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1\end{pmatrix}^{T}) + 
                 (\begin{pmatrix} -2 * 10^{-5} \\ -2 * 10^{-5} \\ -2 * 10^{-5}  \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 0 \\ 0 \\ -1\end{pmatrix}^{T}) = \\
             &= \begin{pmatrix} -0.18721 & -0.18719 & -0.18719 & -0.18717\\ -0.33589 & -0.33587 & -0.33587 & -0.33585 \\ -0.18721 & -0.18719 & -0.18719 & -0.18717\end{pmatrix}
          \end{align*}

          \textbf{The final step} is to calculate the final weights and bias, which shall be given by the following equations:
          
          \begin{equation}\label{ex2-new-weight}
            W^{[i]}_{new} = W^{[i]}_{old} - \eta \nabla\mathcal{L}
          \end{equation}

          \begin{equation}\label{ex2-new-bias}
            b^{[i]}_{new} = b^{[i]}_{old} - \eta \sum_{k=1}^{2}(\delta_k^{i}) 
          \end{equation}

          Considering $\eta = 0.1$ we have all the values we need to start computing the results.
          
          \textbf{By substituting} the values into the formula \ref{ex2-new-weight}, we obtain the following weights:

          \begin{align*}
            W^{[1]}_{new} &= \begin{pmatrix} 1.01872 & 1.01872 & 1.01872 & 1.01872 \\ 1.03359 & 1.03359 & 2.03359 & 1.03359 \\ 1.01872 & 1.01872 & 1.01872 & 1.01872 \end{pmatrix} \\
            W^{[2]}_{new} &= \begin{pmatrix} 1.0173 & 4.02852 & 1.0173 \\ 1.00468 & 1.00772 & 1.00468\end{pmatrix} \\
            W^{[3]}_{new} &= \begin{pmatrix} 0.99704 & 0.99775 \\ 3.01431 & 0.98169 \\ 0.99971 & 1.00041\end{pmatrix} \\
          \end{align*}

          \textbf{Finally,} by replacing the values on the equation \ref{ex2-new-bias}, we get the following bias:

          \begin{align*}
            b^{[1]}_{new} &= \begin{pmatrix} 1.01872 \\ 1.03359 \\ 1.01872\end{pmatrix} \\
            b^{[2]}_{new} &= \begin{pmatrix} 1.03745 \\ 1.01017\end{pmatrix} \\
            b^{[3]}_{new} &= \begin{pmatrix} 1.00198 \\ 1.03177 \\ 0.9993\end{pmatrix} \\
          \end{align*}

\end{enumerate}

\vskip 0.5cm

\newpage

\begin{center}
\large{\textbf{Part II}: Programming and critical analysis}\normalsize
\end{center}

\noindent Considering the \texttt{winequality-red.csv} dataset (available at the webpage) where the goal is  to estimate the quality (sensory appreciation)
of a wine based on physicochemical inputs.

\vskip 0.3cm

\noindent Using a 80-20 training-test split with a fixed seed (\texttt{random\_state = 0}), you are asked to learn MLP
regressors to answer the following questions.

\noindent Given their stochastic behavior, average the performance of each MLP from 10 runs (for reproducibility consider seeding the MLPs with
\texttt{random\_state $\in$ \{1..10\}}).

\begin{enumerate}[leftmargin=\labelsep]
    \item \textbf{Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation
          on all nodes, and early stopping with 20\% of training data set aside for validation. All
          remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as
          default. Plot the distribution of the residues (in absolute value) using a histogram.}

          \vskip 0.3cm
          Blah

    \item \textbf{Since we are in the presence of a integer regression task, a recommended trick is to
          round and bound estimates. Assess the impact of these operations on the MAE of the MLP learnt in previous question.}

          \vskip 0.3cm
          Blah

    \item \textbf{Similarly assess the impact on RMSE from replacing early stopping by a well-defined
          number of iterations in \{20,50,100,200\} (where one iteration corresponds to a batch).}

           \vskip 0.3cm
           Blah

    \item \textbf{Critically comment the results obtained in previous question, hypothesizing at least
          one reason why early stopping favors and/or worsens performance}

          \vskip 0.3cm
          Blah
\end{enumerate}
\end{document}
